ETL-TOOL(1)                 User Commands                  ETL-TOOL(1)

NAME
       etl-tool - A flexible command-line tool for Extract, Transform,
       Load (ETL) operations.

SYNOPSIS
       etl-tool [OPTIONS]

DESCRIPTION
       etl-tool is a configuration-driven utility designed to read data
       from various sources, apply transformations and validations,
       optionally flatten list fields into separate records, filter
       records, handle duplicates, and load the resulting data into various
       destinations. It supports common file formats and PostgreSQL
       databases. The entire process is controlled via a YAML
       configuration file, allowing for complex ETL workflows to be
       defined declaratively.

       The core workflow follows these steps:
       1. Extract: Read data from the configured source (file or database).
       2. Filter: Optionally remove records based on a filter expression
          applied to the source data.
       3. Transform: Apply a series of mapping rules to modify, validate,
          and restructure the data fields.
       4. Flatten: Optionally expand records based on a list/slice field
          into multiple records.
       5. Deduplicate: Optionally remove duplicate records based on
          specified key fields and strategy, applied after transformations
          and flattening.
       6. Load: Write the final processed records to the configured
          destination (file or database).

OPTIONS
       The following command-line options are available:

       -config string
              Specifies the path to the YAML configuration file that defines
              the ETL process. Environment variables in the path will be
              expanded. Defaults to "config/etl-config.yaml".

       -input string
              Overrides the input file path specified in the source.file
              section of the configuration file. This option is ignored if
              the source type is 'postgres'. Environment variables in the
              path will be expanded.

       -output string
              Overrides the output file path or target table specified in the
              destination.file or destination.target_table section of the
              configuration file. This option is ignored if the destination
              type is 'postgres' when overriding target_table. Environment
              variables in the path will be expanded.

       -db string
              Specifies the PostgreSQL connection string (e.g.,
              "postgres://user:password@host:port/database"). This overrides
              the DB_CREDENTIALS environment variable if both are set.
              Required if source or destination type is 'postgres' and
              DB_CREDENTIALS is not set. Environment variables in the
              string will be expanded. Passwords included here will be
              masked in log output.

       -loglevel string
              Sets the logging verbosity level. Valid levels are
              (case-insensitive): none, error, warn (or warning), info,
              debug. Overrides the level set in the configuration file.
              Defaults to "info".

       -dry-run
              If set, the tool performs all steps (extraction, filtering,
              transformation, flattening, deduplication) but skips the final
              load step. This is useful for testing configuration and
              transformations without modifying the destination. Log output
              will indicate the number of records that would have been
              written. Defaults to false.

       -fips
              Enables FIPS 140-2 compliance mode. This restricts certain
              cryptographic algorithms, notably preventing the use of MD5
              hashing in transformations. Overrides the fipsMode setting in
              the configuration file. Defaults to false.

       -help
              Displays the help message summarizing usage, options, and
              environment variables, then exits.

ENVIRONMENT
       DB_CREDENTIALS
              Specifies the PostgreSQL connection string. Used if the -db
              command-line flag is not provided when connecting to PostgreSQL.
              Environment variables within this string will also be expanded.
              Passwords included here will be masked in log output.

       Universal Variable Expansion
              File paths and connection strings specified in the configuration
              file or via command-line flags can contain environment variables.
              The tool supports Unix-style ($VAR, ${VAR}) and Windows-style
              (%VAR%) expansion. If a variable is not set in the environment,
              it is replaced with an empty string.

CONFIGURATION
       The ETL process is defined by a YAML configuration file, typically
       specified via the -config flag. The structure is as follows:

         logging:
           # Optional: Configuration for logging output.
           level: string
             # Sets the logging verbosity. Options: "none", "error", "warn", "info", "debug".
             # Defaults to "info". Can be overridden by the -loglevel flag.

         source:
           # Required: Defines the data source.
           type: string
             # Required: The type of the data source. Supported types:
             #   json: Reads a JSON file containing an array of objects or a single object.
             #   csv:  Reads a Comma Separated Values file. Assumes the first row is the header.
             #   xlsx: Reads data from a Microsoft Excel (.xlsx) file.
             #   xml:  Reads data from an XML file, expecting repeating elements representing records.
             #   yaml: Reads a YAML file containing a list of mappings (records) or a single mapping.
             #   postgres: Reads data by executing a SQL query against a PostgreSQL database.
           file: string
             # Required for file types (json, csv, xlsx, xml, yaml). Path to the input file.
             # Ignored for 'postgres'. Environment variables are expanded. Can be overridden by the -input flag.
           query: string
             # Required for 'postgres' type. The SQL query to execute. Ignored for file types.
           delimiter: string (CSV specific)
             # The single character used as a field delimiter in CSV files. Use '\t' for tab. Defaults to ",".
           commentChar: string (CSV specific)
             # A single character indicating comment lines in CSV files. Lines starting with this are ignored. Defaults to disabled (empty string).
           sheetName: string (XLSX specific)
             # The name of the sheet to read from. Takes precedence over sheetIndex. If neither is specified, reads from the active/first sheet.
           sheetIndex: integer (XLSX specific)
             # The 0-based index of the sheet to read from. Used only if sheetName is not specified. Defaults to the active/first sheet index (usually 0).
           xmlRecordTag: string (XML specific)
             # The local name of the XML elements representing records. Defaults to "record".

         destination:
           # Required: Defines the data destination.
           type: string
             # Required: The type of the data destination. Supported types:
             #   json: Writes records as a JSON array to a file.
             #   csv:  Writes records to a CSV file, with the first row as the header.
             #   xlsx: Writes records to a sheet in a Microsoft Excel (.xlsx) file.
             #   xml:  Writes records as repeating elements within a root element to an XML file.
             #   yaml: Writes records as a YAML list (sequence of mappings) to a file.
             #   postgres: Writes records to a table in a PostgreSQL database. Uses high-performance COPY FROM by default.
           file: string
             # Required for file types (json, csv, xlsx, xml, yaml). Path to the output file.
             # Ignored for 'postgres'. Environment variables are expanded. Can be overridden by the -output flag.
           target_table: string
             # Required for 'postgres' type. Name of the target table (optionally schema-qualified, e.g., "public.my_table").
             # Ignored for file types. Can be overridden by the -output flag.
           delimiter: string (CSV specific)
             # The single character used as a field delimiter when writing CSV. Use '\t' for tab. Defaults to ",".
           sheetName: string (XLSX specific)
             # The name of the sheet to write to. Defaults to "Sheet1". Overwrites if exists.
           xmlRecordTag: string (XML specific)
             # The local name for XML elements representing records in output. Defaults to "record".
           xmlRootTag: string (XML specific)
             # The local name for the root XML element. Defaults to "records".
           loader: (Postgres specific)
             # Optional configuration for PostgreSQL loading behavior. If omitted or mode is empty/invalid, uses COPY FROM.
             mode: string
               # Specifies the loading strategy. Currently only supports "sql". Defaults to "" (uses COPY).
             command: string
               # Required if mode="sql". SQL command (e.g., INSERT, UPDATE) executed per record.
               # Use placeholders like $1, $2 corresponding to alphabetical order of target field names.
             preload: array of strings
               # Optional (mode="sql"): SQL commands executed once *before* custom SQL loading (e.g., TRUNCATE).
             postload: array of strings
               # Optional (mode="sql"): SQL commands executed once *after* custom SQL loading (e.g., ANALYZE).
             batch_size: integer
               # Optional (mode="sql"): Number of records per transaction batch. <= 0 disables batching. Default 0.

         filter: string
           # Optional: Expression (govaluate syntax) evaluated against each *input* record *before* transformations.
           # Records evaluating to false are skipped. Example: "status == 'active' && amount > 0"

         mappings:
           # Required: Array of rules defining transformations and validations applied sequentially.
           - source: string
               # Required: Name of the field in the input record (or previous target field).
             target: string
               # Required: Name of the field in the output record. Must be unique across mappings.
             transform: string
               # Optional: Name of the transformation/validation function. Can include shorthand param (e.g., "regexExtract:pattern").
               # If omitted, source value is assigned directly. Available functions:
               #
               #   toString: Converts input value to its string representation. Handles nil as "".
               #   toInt: Attempts to convert input value (string, float, int types) to an int64. Returns nil on failure.
               #   mustToInt: Converts input value to an int64. Returns an error if conversion fails, triggering error handling (halt/skip).
               #   toFloat: Attempts to convert input value (string, float, int types) to a float64. Returns nil on failure.
               #   mustToFloat: Converts input value to a float64. Returns an error if conversion fails.
               #   toBool: Attempts to convert input value (string, numeric, bool) to a boolean. Recognizes "true", "t", "yes", "y", "1" (and case variations) as true; "false", "f", "no", "n", "0", "" as false. Returns nil for unrecognized strings. Non-zero numbers are true. Nil is false.
               #   mustToBool: Converts input value to a boolean using the same rules as toBool, but returns an error for nil, empty string, or unrecognized string values.
               #   toUpperCase: Converts a string value to uppercase. Non-strings pass through.
               #   toLowerCase: Converts a string value to lowercase. Non-strings pass through.
               #   trim: Removes leading and trailing whitespace from a string value. Non-strings pass through.
               #   epochToDate: Converts a numeric Unix epoch timestamp (seconds, can be float) to a date string in "YYYY-MM-DD" format (UTC). Returns original value on parse failure.
               #   mustEpochToDate: Converts a numeric Unix epoch timestamp to "YYYY-MM-DD" format. Returns an error if conversion fails.
               #   dateConvert: Converts a date/time string or time.Time object from one format to another. Uses parameters `inputFormat` (Go layout string, defaults to RFC3339 and common fallbacks) and `outputFormat` (Go layout string, defaults to RFC3339). Returns original value on parse failure.
               #   mustDateConvert: Converts a date/time string or time.Time object using `inputFormat` and `outputFormat`. Returns an error if parsing fails.
               #   multiDateConvert: Attempts to parse a date string using multiple potential input formats specified in the `formats` parameter (an array of Go layout strings). Returns the formatted date (using `outputFormat`) on the first successful parse, or the original value if none match. Requires `formats` and `outputFormat` params.
               #   calculateAge: Calculates the age in *days* between a Unix epoch timestamp (seconds) and the current time (UTC). Returns an integer number of days, or nil on parse failure. Returns 0 for future dates.
               #   replaceAll: Replaces all occurrences of a substring within a string. Requires `old` and `new` string parameters. Non-strings pass through.
               #   substring: Extracts a portion of a string. Requires `start` (0-based index) and `length` integer parameters. Handles multi-byte characters correctly. Returns original value if input is not a string or params are invalid.
               #   regexExtract: Extracts the first capture group from a string using a regular expression. Requires a `pattern` string parameter (or shorthand: "regexExtract:pattern"). Returns the captured string or nil if no match or capture group exists, or on pattern error.
               #   hash: Generates a cryptographic hash (hex string) of the concatenated string representations of values from specified fields. Requires `algorithm` (string: "sha256", "sha512", "md5") and `fields` (array of strings) parameters. Fields are sorted alphabetically before concatenation. MD5 is disallowed if FIPS mode is enabled.
               #   coalesce: Returns the first non-nil value from a list of fields specified in the `fields` parameter (an array of strings). If the value is a string, it must also be non-empty. Returns nil if no suitable value is found.
               #   branch: Evaluates conditions sequentially and returns a corresponding value. Requires a `branches` parameter, which is an array of maps. Each map must contain a `condition` (string, govaluate expression) and a `value` (any type). Returns the `value` from the first branch whose `condition` evaluates to true. If no condition matches, returns the original input value passed to the transform. Uses `inputValue` to refer to the input value in conditions, and other record fields by name.
               #   validateRequired: Returns an error if the input value is nil, an empty string, or a whitespace-only string. Otherwise, returns the original value.
               #   validateRegex: Returns an error if the input string value does not match the provided regular expression pattern. Requires a `pattern` string parameter (or shorthand: "validateRegex:pattern"). Non-string values pass validation.
               #   validateNumericRange: Returns an error if the input numeric value is outside the specified range. Requires at least one of `min` or `max` numeric parameters. Non-numeric values pass validation.
               #   validateAllowedValues: Returns an error if the input value is not present in the specified list. Requires a `values` array parameter. Comparison uses type-aware logic (e.g., int 10 matches string "10").
             params: map
               # Optional: Map of additional parameters for the function (e.g., date formats, regex pattern, validation rules).

         flattening:
           # Optional: Configuration to expand records based on a list/slice field.
           # Occurs *after* mapping/transformation and *before* deduplication.
           sourceField: string
             # Required: Field containing the list/slice to flatten (dot-notation supported, e.g., "details.items").
           targetField: string
             # Required: Name of the field in the output record where each list item will be placed.
           includeParent: boolean
             # Optional: If true (default), copies parent record fields into each flattened record.
           errorOnNonList: boolean
             # Optional: If true, errors if sourceField is not a list/slice. If false (default), skips record.
           conditionField: string
             # Optional: Flatten only if this parent field's value matches conditionValue.
           conditionValue: string
             # Optional: Required value for conditionField if conditionField is set.

         dedup:
           # Optional: Configuration for removing duplicate records. Applied *after* transformations and flattening.
           keys: array of strings
             # Required: List of target field names forming the composite key.
           strategy: string
             # Optional: How to select the record to keep. Options:
             #   "first": (Default) Keep the first record encountered.
             #   "last": Keep the last record encountered.
             #   "min": Keep record with minimum value in strategyField.
             #   "max": Keep record with maximum value in strategyField.
           strategyField: string
             # Optional: Required if strategy is "min" or "max". Target field for comparison.

         errorHandling:
           # Optional: Configuration defining how record-level processing errors are handled.
           mode: string
             # Mode: "halt" (default, stop on first error) or "skip" (log/write error, continue).
           logErrors: boolean
             # Optional (mode="skip"): If true (default when mode=skip), log skipped records/errors.
           errorFile: string
             # Optional (mode="skip"): Path to a CSV file where skipped records (original data + error) will be appended. Environment variables are expanded.

         fipsMode: boolean
           # Optional: If true, enables FIPS compliance mode (restricts MD5). Defaults to false. Can be overridden by the -fips flag.

EXAMPLES
       1. Basic CSV to JSON conversion:

          # config.yaml
          source:
            type: csv
            file: input.csv
          destination:
            type: json
            file: output.json
          mappings:
            - { source: user_id, target: userId }
            - { source: email_address, target: email }
            - { source: value, target: amount, transform: toFloat }

          etl-tool -config config.yaml

       2. Reading from PostgreSQL, filtering, and writing to XLSX:

          # pg_to_xlsx.yaml
          source:
            type: postgres
            query: "SELECT product_id, name, category, price, created_at FROM products"
          destination:
            type: xlsx
            file: /data/output/active_products.xlsx
            sheetName: Active Products
          filter: "category != 'discontinued' && price > 0" # Filter before transform
          mappings:
            - { source: product_id, target: ProductID }
            - { source: name, target: ProductName, transform: toUpperCase }
            - { source: price, target: Price }
            - { source: created_at, target: CreatedDate, transform: dateConvert, params: { outputFormat: "2006-01-02" } }

          etl-tool -config pg_to_xlsx.yaml -db "postgres://user:pass@host/db"

       3. Transforming data, skipping errors, and writing errors to file:

          # transform_skip.yaml
          source:
            type: json
            file: raw_data.json
          destination:
            type: csv
            file: processed_data.csv
          errorHandling:
            mode: skip
            logErrors: true
            errorFile: errors.csv
          mappings:
            - { source: id, target: record_id, transform: mustToInt } # Error if not int
            - { source: event_time, target: event_date, transform: mustEpochToDate } # Error if invalid epoch
            - { source: status_code, target: status, transform: toString }
            - { source: email, target: email, transform: validateRegex:[a-z]+@[a-z]+\.[a-z]+ } # Error if invalid

          etl-tool -config transform_skip.yaml

       4. Flattening a list and deduplicating results:

          # flatten_dedup.yaml
          source:
            type: json
            file: input_lists.json # e.g., [{"id": 1, "tags": ["A", "B"]}, {"id": 2, "tags": ["B", "C"]}]
          destination:
            type: json
            file: output_flat.json
          mappings: # Map original fields first
            - { source: id, target: recordId }
            - { source: tags, target: tagList } # Keep the list temporarily
          flattening:
            sourceField: tagList # Flatten the list mapped above
            targetField: tag # Each item goes into the 'tag' field
            includeParent: true # Keep 'recordId'
          dedup:
            keys: ["tag"] # Deduplicate based on the flattened tag value
            strategy: first # Keep first occurrence of each tag

          # Expected output: [{"recordId": 1, "tag": "A"}, {"recordId": 1, "tag": "B"}, {"recordId": 2, "tag": "C"}]
          etl-tool -config flatten_dedup.yaml

       5. Using environment variables:

          # env_var_config.yaml
          source:
            type: csv
            file: "$INPUT_DIR/data.csv" # Unix style
          destination:
            type: postgres
            target_table: "%TARGET_TABLE%" # Windows style
          mappings:
            - { source: col, target: dest_col }

          export INPUT_DIR=/mnt/data
          export TARGET_TABLE=final_results
          etl-tool -config env_var_config.yaml -db "$DB_CONN_STR"

FILES
       config/etl-config.yaml
              The default path searched for the configuration file if the
              -config flag is not specified.

EXIT STATUS
       0
              Successful completion.
       1
              An error occurred during execution (e.g., configuration error,
              processing error in halt mode, file I/O error). Details are
              typically logged to standard error or the configured error file.

BUGS
       Report bugs to the project maintainer. Ensure FIPS mode is used
       appropriately based on security requirements. Ensure PostgreSQL loader
       configuration (especially custom SQL) is secure and correct.

AUTHOR
       Brian Moore

COPYRIGHT
       Refer to the LICENSE file distributed with this software.

etl-tool                      April 9, 2025                   ETL-TOOL(1)
